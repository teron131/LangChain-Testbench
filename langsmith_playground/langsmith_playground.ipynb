{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For other dataset creation methods, see:\n",
    "# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically\n",
    "# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application\n",
    "\n",
    "# Create inputs and reference outputs\n",
    "examples = [\n",
    "    (\n",
    "        \"Which country is Mount Kilimanjaro located in?\",\n",
    "        \"Mount Kilimanjaro is located in Tanzania.\",\n",
    "    ),\n",
    "    (\n",
    "        \"What is Earth's lowest point?\",\n",
    "        \"Earth's lowest point is The Dead Sea.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "inputs = [{\"question\": input_prompt} for input_prompt, _ in examples]\n",
    "outputs = [{\"answer\": output_answer} for _, output_answer in examples]\n",
    "\n",
    "try:\n",
    "    # Programmatically create a dataset in LangSmith\n",
    "    dataset = client.create_dataset(dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\")\n",
    "\n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the application logic you want to evaluate inside a target function\n",
    "# The SDK will automatically send the inputs from the dataset to your target function\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "def target(inputs: dict) -> dict:\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"user\", \"{question}\")])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"question\": inputs[\"question\"]})\n",
    "    return {\"response\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instructions for the LLM judge evaluator\n",
    "instructions = \"\"\"Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: \n",
    "- False: No conceptual match and similarity\n",
    "- True: Most or full conceptual match and similarity\n",
    "- Key criteria: Concept should match, not exact wording.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define output schema for the LLM judge\n",
    "class Grade(BaseModel):\n",
    "    score: bool = Field(description=\"Boolean that indicates whether the response is accurate relative to the reference answer\")\n",
    "\n",
    "\n",
    "# Define LLM judge that grades the accuracy of the response relative to reference output\n",
    "def accuracy(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", instructions),\n",
    "            (\"user\", \"Ground Truth answer: {answer}; Student's Answer: {response}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    chain = prompt | llm.with_structured_output(Grade)\n",
    "    response = chain.invoke({\"answer\": reference_outputs[\"answer\"], \"response\": outputs[\"response\"]})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=True\n"
     ]
    }
   ],
   "source": [
    "# Test with dummy data\n",
    "test_outputs = {\"response\": \"The Earth revolves around the Sun\"}\n",
    "test_reference = {\"answer\": \"Our planet Earth orbits the Sun in an elliptical path\"}\n",
    "res = accuracy(test_outputs, test_reference)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'first-eval-in-langsmith-c36b9a4e' at:\n",
      "https://smith.langchain.com/o/eb122562-97bd-51d4-9e3f-86c9acffa2bc/datasets/5089c08c-d8d9-41fe-9474-f412e34bdcec/compare?selectedSessions=852bf841-39cc-4d60-89d9-70b6cff9ef1b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d1ea8fc78b488b86671b8f7ab00520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# After running the evaluation, a link will be provided to view the results in langsmith\n",
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=\"Sample dataset\",\n",
    "    evaluators=[\n",
    "        accuracy,\n",
    "        # can add multiple evaluators here\n",
    "    ],\n",
    "    experiment_prefix=\"first-eval-in-langsmith\",\n",
    "    max_concurrency=2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
